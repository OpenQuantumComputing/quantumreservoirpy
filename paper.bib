@InProceedings{fernando03,
author="Fernando, Chrisantha
and Sojakka, Sampsa",
title="Pattern Recognition in a Bucket",
booktitle="Advances in Artificial Life",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="588--597",
abstract="This paper demonstrates that the waves produced on the surface of water can be used as the medium for a ``Liquid State Machine'' that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass' Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this ``for free'', and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.",
isbn="978-3-540-39432-7"
}

@InProceedings{trouvain20,
author="Trouvain, Nathan
and Pedrelli, Luca
and Dinh, Thanh Trung
and Hinaut, Xavier",
editor="Farka{\v{s}}, Igor
and Masulli, Paolo
and Wermter, Stefan",
title="ReservoirPy: An Efficient and User-Friendly Library to Design Echo State Networks",
booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="494--505",
abstract="We present a simple user-friendly library called ReservoirPy based on Python scientific modules. It provides a flexible interface to implement efficient Reservoir Computing (RC) architectures with a particular focus on Echo State Networks (ESN). Advanced features of ReservoirPy allow to improve upÂ to {\$}{\$}87.9{\backslash}{\%}{\$}{\$}of computation time efficiency on a simple laptop compared to basic Python implementation. Overall, we provide tutorials for hyperparameters tuning, offline and online training, fast spectral initialization, parallel and sparse matrix computation on various tasks (MackeyGlass and audio recognition tasks). In particular, we provide graphical tools to easily explore hyperparameters using random search with the help of the hyperopt library.",
isbn="978-3-030-61616-8"
}

@article{jaeger04,
author = {Herbert Jaeger  and Harald Haas },
title = {Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication},
journal = {Science},
volume = {304},
number = {5667},
pages = {78-80},
year = {2004},
doi = {10.1126/science.1091277},
URL = {https://www.science.org/doi/abs/10.1126/science.1091277},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1091277},
abstract = {We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.}}

@Article{mujal23,
author={Mujal, Pere
and Mart{\'i}nez-Pe{\~{n}}a, Rodrigo
and Giorgi, Gian Luca
and Soriano, Miguel C.
and Zambrini, Roberta},
title={Time-series quantum reservoir computing with weak and projective measurements},
journal={npj Quantum Information},
year={2023},
month={2},
day={23},
volume={9},
number={1},
pages={16},
abstract={Time-series processing is a major challenge in machine learning with enormous progress in the last years in tasks such as speech recognition and chaotic series prediction. A promising avenue for sequential data analysis is quantum machine learning, with computational models like quantum neural networks and reservoir computing. An open question is how to efficiently include quantum measurement in realistic protocols while retaining the needed processing memory and preserving the quantum advantage offered by large Hilbert spaces. In this work, we propose different measurement protocols and assess their efficiency in terms of resources, through theoretical predictions and numerical analysis. We show that it is possible to exploit the quantumness of the reservoir and to obtain ideal performance both for memory and forecasting tasks with two successful measurement protocols. One repeats part of the experiment after each projective measurement while the other employs weak measurements operating online at the trade-off where information can be extracted accurately and without hindering the needed memory, in spite of back-action effects. Our work establishes the conditions for efficient time-series processing paving the way to its implementation in different quantum technologies.},
issn={2056-6387},
doi={10.1038/s41534-023-00682-z},
url={https://doi.org/10.1038/s41534-023-00682-z}
}

@Article{suzuki22,
  author={Suzuki, Yudai
  and Gao, Qi
  and Pradel, Ken C.
  and Yasuoka, Kenji
  and Yamamoto, Naoki},
  title={Natural quantum reservoir computing for temporal information processing},
  journal={Scientific Reports},
  year={2022},
  month={1},
  day={25},
  volume={12},
  number={1},
  pages={1353},
  abstract={Reservoir computing is a temporal information processing system that exploits artificial or physical dissipative dynamics to learn a dynamical system and generate the target time-series. This paper proposes the use of real superconducting quantum computing devices as the reservoir, where the dissipative property is served by the natural noise added to the quantum bits. The performance of this natural quantum reservoir is demonstrated in a benchmark time-series regression problem and a practical problem classifying different objects based on temporal sensor data. In both cases the proposed reservoir computer shows a higher performance than a linear regression or classification model. The results indicate that a noisy quantum device potentially functions as a reservoir computer, and notably, the quantum noise, which is undesirable in the conventional quantum computation, can be used as a rich computation resource.},
  issn={2045-2322},
  doi={10.1038/s41598-022-05061-w},
  url={https://doi.org/10.1038/s41598-022-05061-w}
}

@Misc{yasuda23,
      title={Quantum reservoir computing with repeated measurements on superconducting devices},
      author={Toshiki Yasuda and Yudai Suzuki and Tomoyuki Kubota and Kohei Nakajima and Qi Gao and Wenlong Zhang and Satoshi Shimono and Hendra I. Nurdin and Naoki Yamamoto},
      year={2023},
      eprint={2310.06706},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

@Article{tanaka2019,
  title={Recent advances in physical reservoir computing: A review},
  author={Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  journal={Neural Networks},
  volume={115},
  pages={100--123},
  year={2019},
  publisher={Elsevier}
}


